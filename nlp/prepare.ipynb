{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = articles[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = article\n",
    "article = article.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "normalized = unicodedata.normalize('NFKD', article)\n",
    "\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropps non-ascii\n",
    "just_ascii = article.encode('ascii', 'ignore')\n",
    "# xform back to ascii\n",
    "article = just_ascii.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization by hand, not fun and a few errors\n",
    "\n",
    "import re\n",
    "\n",
    "' '.join(re.sub(r'[^\\w\\s]',' ', article).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with nltk! Much better!\n",
    "\n",
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "word_list = tokenizer.tokenize(article)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = tokenizer.tokenize(article, return_str=True)\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "[ps.stem(word) for word in ['called', 'calling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([ps.stem(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'study studies'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n",
    "\n",
    "list_of_blog = acquire.get_blog_articles()\n",
    "list_of_news = acquire.get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    article = re.sub(r'\\s', ' ', article).lower()\n",
    "    article = unicodedata.normalize('NFKD', article)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "    article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "    return article\n",
    "\n",
    "\n",
    "def tokenize(article):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    article = tokenizer.tokenize(article, return_str=True)\n",
    "    return article\n",
    "    \n",
    "\n",
    "def stem(article):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed\n",
    "\n",
    "\n",
    "def lemmatize(article):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized\n",
    "\n",
    "\n",
    "def remove_stopwords(article, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    [stopword_list.append(word) for word in extra_words if word not in stopword_list]\n",
    "    [stopword_list.remove(word) for word in exclude_words if word in stopword_list]\n",
    "    words = article.lower().split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords\n",
    "\n",
    "\n",
    "def prep_article(dictionary_article, extra_words = [], exclude_words = []):\n",
    "    if 'category' in dictionary_article:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'category': dictionary_article['category'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "        \n",
    "    else:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "\n",
    "    return cleaned_dict\n",
    "\n",
    "def prepare_article_data(dictionary_article, extra_words = [], exclude_words = []):\n",
    "    clean_dict_list = []\n",
    "    \n",
    "    for artical in dictionary_article:\n",
    "        clean_dict_list.append(prep_article(artical, extra_words, exclude_words))\n",
    "        \n",
    "    return clean_dict_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = ['codeup']\n",
    "exclude_words = ['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = prepare_article_data(list_of_blog, extra_words, exclude_words)\n",
    "news = prepare_article_data(list_of_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def basic_clean(article):\n",
    "#     article = re.sub(r'\\s', ' ', article).lower()\n",
    "#     article = unicodedata.normalize('NFKD', article)\\\n",
    "#         .encode('ascii', 'ignore')\\\n",
    "#         .decode('utf-8', 'ignore')\n",
    "#     article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "#     return article\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(article):\n",
    "#     tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "#     article = tokenizer.tokenize(article, return_str=True)\n",
    "#     return article\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem(article):\n",
    "#     ps = nltk.porter.PorterStemmer()\n",
    "#     stems = [ps.stem(word) for word in article.split()]\n",
    "#     article_stemmed = ' '.join(stems)\n",
    "#     return article_stemmed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize(article):\n",
    "#     wnl = nltk.stem.WordNetLemmatizer()\n",
    "#     lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "#     article_lemmatized = ' '.join(lemmas)\n",
    "#     return article_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = ['codeup']\n",
    "exclude_words = ['the']\n",
    "\n",
    "def remove_stopwords(article, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    [stopword_list.append(word) for word in extra_words if word not in stopword_list]\n",
    "    [stopword_list.remove(word) for word in exclude_words if word in stopword_list]\n",
    "    words = article.lower().split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords\n",
    "\n",
    "remove_stopwords(list_of_blog[0]['content'], extra_words, exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article(dictionary_article, extra_words = [], exclude_words = []):\n",
    "    if 'category' in dictionary_article:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'category': dictionary_article['category'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "        \n",
    "    else:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "\n",
    "    return cleaned_dict\n",
    "    \n",
    "prep_article(list_of_blog[0], extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_article_data(dictionary_article, extra_words = [], exclude_words = []):\n",
    "#     clean_dict_list = []\n",
    "    \n",
    "#     for artical in dictionary_article:\n",
    "#         clean_dict_list.append(prep_article(artical))\n",
    "        \n",
    "#     return clean_dict_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
