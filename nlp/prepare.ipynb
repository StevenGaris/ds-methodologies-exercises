{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = articles[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = article\n",
    "article = article.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "normalized = unicodedata.normalize('NFKD', article)\n",
    "\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropps non-ascii\n",
    "just_ascii = article.encode('ascii', 'ignore')\n",
    "# xform back to ascii\n",
    "article = just_ascii.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization by hand, not fun and a few errors\n",
    "\n",
    "import re\n",
    "\n",
    "' '.join(re.sub(r'[^\\w\\s]',' ', article).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with nltk! Much better!\n",
    "\n",
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "word_list = tokenizer.tokenize(article)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = tokenizer.tokenize(article, return_str=True)\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "[ps.stem(word) for word in ['called', 'calling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([ps.stem(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'study studies'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n",
    "\n",
    "list_of_blog = acquire.get_blog_articles()\n",
    "list_of_news = acquire.get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    article = re.sub(r'\\s', ' ', article).lower()\n",
    "    article = unicodedata.normalize('NFKD', article)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "    article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "    return article\n",
    "\n",
    "\n",
    "def tokenize(article):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    article = tokenizer.tokenize(article, return_str=True)\n",
    "    return article\n",
    "    \n",
    "\n",
    "def stem(article):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed\n",
    "\n",
    "\n",
    "def lemmatize(article):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized\n",
    "\n",
    "\n",
    "def remove_stopwords(article, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    [stopword_list.append(word) for word in extra_words if word not in stopword_list]\n",
    "    [stopword_list.remove(word) for word in exclude_words if word in stopword_list]\n",
    "    words = article.lower().split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords\n",
    "\n",
    "\n",
    "def prep_article(dictionary_article, extra_words = [], exclude_words = []):\n",
    "    if 'category' in dictionary_article:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'category': dictionary_article['category'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "        \n",
    "    else:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "\n",
    "    return cleaned_dict\n",
    "\n",
    "def prepare_article_data(dictionary_article, extra_words = [], exclude_words = []):\n",
    "    clean_dict_list = []\n",
    "    \n",
    "    for artical in dictionary_article:\n",
    "        clean_dict_list.append(prep_article(artical, extra_words, exclude_words))\n",
    "        \n",
    "    return clean_dict_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = ['codeup']\n",
    "exclude_words = ['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = prepare_article_data(list_of_blog, extra_words, exclude_words)\n",
    "news = prepare_article_data(list_of_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumors true the time arrived officially opened applications new data science career accelerator 25 seats available immersive program one kind san antonio help land job glassdoors 1 best job america data science method providing actionable intelligence data the data revolution hit san antonio resulting explosion data scientist positions across companies like usaa accenture booz allen hamilton heb weve even seen utsa invest 70 cybersecurity center school data science built program specifically meet the growing demands industry program 18 weeks long fulltime handson projectbased curriculum development instruction led senior data scientist maggie giust worked heb capital group rackspace along input dozens practitioners hiring partners students work real data sets realistic problems the entire data science pipeline collection deployment receive professional development training resume writing interviewing continuing education prepare smooth transition the workforce focus applied data science immediate impact roi business back 6 month tuition refund guarantee like existing web dev program focusing data science python sql ml covered 14 modules 1 fundamentals 2 applied statistics 3 sql 4 python 5 supervised machine learning regression 6 supervised machine learning classification 7 unsupervised machine learning clustering 8 time series analysis 9 anomaly detection 10 natural language processing 11 distributed machine learning 12 advanced topics deep learning nosql cloud deployment etc 13 storytelling data 14 domain expertise development applications open codeups first data science cohort start class february 4 2019 hurry 25 seats available mission cultivating inclusive growth scholarships available women minorities lgbtqia individuals veterans first responders people relocating san antonio want learn joining program hiring graduates email datasciencecodeupcom'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog[0]['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def basic_clean(article):\n",
    "#     article = re.sub(r'\\s', ' ', article).lower()\n",
    "#     article = unicodedata.normalize('NFKD', article)\\\n",
    "#         .encode('ascii', 'ignore')\\\n",
    "#         .decode('utf-8', 'ignore')\n",
    "#     article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "#     return article\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(article):\n",
    "#     tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "#     article = tokenizer.tokenize(article, return_str=True)\n",
    "#     return article\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem(article):\n",
    "#     ps = nltk.porter.PorterStemmer()\n",
    "#     stems = [ps.stem(word) for word in article.split()]\n",
    "#     article_stemmed = ' '.join(stems)\n",
    "#     return article_stemmed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize(article):\n",
    "#     wnl = nltk.stem.WordNetLemmatizer()\n",
    "#     lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "#     article_lemmatized = ' '.join(lemmas)\n",
    "#     return article_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = ['codeup']\n",
    "exclude_words = ['the']\n",
    "\n",
    "def remove_stopwords(article, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    [stopword_list.append(word) for word in extra_words if word not in stopword_list]\n",
    "    [stopword_list.remove(word) for word in exclude_words if word in stopword_list]\n",
    "    words = article.lower().split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords\n",
    "\n",
    "remove_stopwords(list_of_blog[0]['content'], extra_words, exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article(dictionary_article, extra_words = [], exclude_words = []):\n",
    "    if 'category' in dictionary_article:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'category': dictionary_article['category'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "        \n",
    "    else:\n",
    "        cleaned_dict = {\n",
    "        'title': dictionary_article['title'],\n",
    "        'original': dictionary_article['content'],\n",
    "        'stemmed': stem(dictionary_article['content']),\n",
    "        'lemmatized': lemmatize(dictionary_article['content']),\n",
    "        'clean': remove_stopwords(basic_clean(dictionary_article['content']), extra_words, exclude_words),\n",
    "            }\n",
    "\n",
    "    return cleaned_dict\n",
    "    \n",
    "prep_article(list_of_blog[0], extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_article_data(dictionary_article, extra_words = [], exclude_words = []):\n",
    "#     clean_dict_list = []\n",
    "    \n",
    "#     for artical in dictionary_article:\n",
    "#         clean_dict_list.append(prep_article(artical))\n",
    "        \n",
    "#     return clean_dict_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
